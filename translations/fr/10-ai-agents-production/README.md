# Agents IA en Production : ObservabilitÃ© et Ã‰valuation

[![Agents IA en Production](../../../translated_images/fr/lesson-10-thumbnail.2b79a30773db093e.webp)](https://youtu.be/l4TP6IyJxmQ?si=reGOyeqjxFevyDq9)

Ã€ mesure que les agents IA passent de prototypes expÃ©rimentaux Ã  des applications rÃ©elles, la capacitÃ© Ã  comprendre leur comportement, surveiller leurs performances et Ã©valuer systÃ©matiquement leurs rÃ©sultats devient importante.

## Objectifs d'apprentissage

AprÃ¨s avoir terminÃ© cette leÃ§on, vous saurez/comment comprendre :
- Les concepts clÃ©s de l'observabilitÃ© et de l'Ã©valuation des agents
- Les techniques pour amÃ©liorer les performances, les coÃ»ts et l'efficacitÃ© des agents
- Ce qu'il faut Ã©valuer et comment Ã©valuer systÃ©matiquement vos agents IA
- Comment contrÃ´ler les coÃ»ts lors du dÃ©ploiement des agents IA en production
- Comment instrumenter des agents construits avec AutoGen

L'objectif est de vous Ã©quiper avec les connaissances nÃ©cessaires pour transformer vos agents "boÃ®te noire" en systÃ¨mes transparents, gÃ©rables et fiables.

_**Note :** Il est important de dÃ©ployer des agents IA sÃ»rs et dignes de confiance. Consultez Ã©galement la leÃ§on [Construire des Agents IA Dignes de Confiance](./06-building-trustworthy-agents/README.md)._

## Traces et Spans

Les outils d'observabilitÃ© tels que [Langfuse](https://langfuse.com/) ou [Microsoft Foundry](https://learn.microsoft.com/en-us/azure/ai-foundry/what-is-azure-ai-foundry) reprÃ©sentent gÃ©nÃ©ralement les exÃ©cutions d'agents sous forme de traces et de spans.

- **Trace** reprÃ©sente une tÃ¢che complÃ¨te de lâ€™agent du dÃ©but Ã  la fin (comme traiter une requÃªte utilisateur).
- **Spans** sont les Ã©tapes individuelles au sein de la trace (comme lâ€™appel Ã  un modÃ¨le de langage ou la rÃ©cupÃ©ration de donnÃ©es).

![Arbre de trace dans Langfuse](https://langfuse.com/images/cookbook/example-autogen-evaluation/trace-tree.png)

Sans observabilitÃ©, un agent IA peut sembler Ãªtre une "boÃ®te noire" â€“ son Ã©tat interne et son raisonnement sont opaques, rendant difficile le diagnostic des problÃ¨mes ou lâ€™optimisation des performances. Avec l'observabilitÃ©, les agents deviennent des "boÃ®tes en verre", offrant une transparence vitale pour instaurer la confiance et assurer quâ€™ils fonctionnent comme prÃ©vu. 

## Pourquoi lâ€™ObservabilitÃ© est Importante en Environnements de Production

La transition des agents IA vers les environnements de production introduit de nouveaux dÃ©fis et exigences. L'observabilitÃ© nâ€™est plus un simple Â« plus Â» mais une capacitÃ© critique :

*   **DÃ©bogage et Analyse des Causes Racines** : Lorsquâ€™un agent Ã©choue ou produit une sortie inattendue, les outils dâ€™observabilitÃ© fournissent les traces nÃ©cessaires pour localiser la source de lâ€™erreur. Ceci est particuliÃ¨rement important dans les agents complexes pouvant impliquer plusieurs appels LLM, interactions avec des outils et logique conditionnelle.
*   **Gestion de la Latence et des CoÃ»ts** : Les agents IA sâ€™appuient souvent sur des LLM et dâ€™autres API externes facturÃ©s Ã  la token ou Ã  lâ€™appel. Lâ€™observabilitÃ© permet de suivre prÃ©cisÃ©ment ces appels, aidant Ã  identifier les opÃ©rations trop lentes ou coÃ»teuses. Cela permet aux Ã©quipes dâ€™optimiser les prompts, choisir des modÃ¨les plus efficaces ou repenser les workflows pour gÃ©rer les coÃ»ts opÃ©rationnels et garantir une bonne expÃ©rience utilisateur.
*   **Confiance, SÃ©curitÃ© et ConformitÃ©** : Dans de nombreuses applications, il est important de sâ€™assurer que les agents se comportent de maniÃ¨re sÃ»re et Ã©thique. Lâ€™observabilitÃ© fournit une traÃ§abilitÃ© des actions et dÃ©cisions des agents. Cela peut Ãªtre utilisÃ© pour dÃ©tecter et attÃ©nuer des problÃ¨mes comme l'injection de prompt, la gÃ©nÃ©ration de contenu nuisible ou la mauvaise gestion des informations personnellement identifiables (PII). Par exemple, vous pouvez revoir les traces pour comprendre pourquoi un agent a donnÃ© une certaine rÃ©ponse ou utilisÃ© un outil spÃ©cifique.
*   **Boucles dâ€™AmÃ©lioration Continue** : Les donnÃ©es dâ€™observabilitÃ© sont la base dâ€™un processus de dÃ©veloppement itÃ©ratif. En surveillant la performance des agents dans le monde rÃ©el, les Ã©quipes peuvent identifier des axes dâ€™amÃ©lioration, collecter des donnÃ©es pour affiner les modÃ¨les et valider lâ€™impact des modifications. Cela crÃ©e une boucle de rÃ©troaction oÃ¹ les insights de la production issus de lâ€™Ã©valuation en ligne alimentent lâ€™expÃ©rimentation et le raffinement hors ligne, menant Ã  une meilleure performance progressive des agents.

## Principaux Indicateurs Ã  Suivre

Pour surveiller et comprendre le comportement des agents, une gamme de mÃ©triques et signaux doit Ãªtre suivie. Bien que les mÃ©triques spÃ©cifiques puissent varier selon lâ€™objectif de lâ€™agent, certaines sont universellement importantes.

Voici quelques-unes des mÃ©triques les plus courantes surveillÃ©es par les outils dâ€™observabilitÃ© :

**Latence :** Ã€ quelle vitesse lâ€™agent rÃ©pond-il ? Des temps dâ€™attente longs impactent nÃ©gativement lâ€™expÃ©rience utilisateur. Vous devez mesurer la latence pour les tÃ¢ches et les Ã©tapes individuelles en retraÃ§ant les exÃ©cutions de lâ€™agent. Par exemple, un agent prenant 20 secondes pour tous les appels modÃ¨les pourrait Ãªtre accÃ©lÃ©rÃ© en utilisant un modÃ¨le plus rapide ou en exÃ©cutant les appels en parallÃ¨le.

**CoÃ»ts :** Quel est le coÃ»t par exÃ©cution dâ€™agent ? Les agents IA sâ€™appuient sur des appels LLM facturÃ©s par token ou des API externes. Une utilisation frÃ©quente dâ€™outils ou plusieurs prompts peuvent rapidement augmenter les coÃ»ts. Par exemple, si un agent appelle un LLM cinq fois pour une amÃ©lioration marginale de qualitÃ©, vous devez Ã©valuer si le coÃ»t est justifiÃ© ou si vous pouvez rÃ©duire le nombre dâ€™appels ou utiliser un modÃ¨le moins cher. La surveillance en temps rÃ©el peut aussi aider Ã  identifier des pics inattendus (ex. bugs causant des boucles API excessives).

**Erreurs de RequÃªte :** Combien de requÃªtes lâ€™agent a-t-il Ã©chouÃ© ? Cela peut inclure des erreurs dâ€™API ou des appels dâ€™outil Ã©chouÃ©s. Pour renforcer la robustesse de votre agent en production, vous pouvez ensuite configurer des solutions de secours ou des rÃ©essais. Par ex. si le fournisseur LLM A est indisponible, vous basculez vers le fournisseur B en secours.

**Retour Utilisateur :** ImplÃ©menter des Ã©valuations directes utilisateurs fournit des informations prÃ©cieuses. Cela peut inclure des Ã©valuations explicites (ğŸ‘pouce levÃ©/ğŸ‘pouce baissÃ©, â­1-5 Ã©toiles) ou des commentaires textuels. Un retour nÃ©gatif constant doit vous alerter, signe que lâ€™agent ne fonctionne pas comme attendu.

**Retour Utilisateur Implicite :** Les comportements utilisateurs fournissent un retour indirect mÃªme sans notes explicites. Cela peut inclure la reformulation immÃ©diate dâ€™une question, des requÃªtes rÃ©pÃ©tÃ©es ou un clic sur un bouton de rÃ©essai. Par ex., si vous constatez que les utilisateurs posent Ã  plusieurs reprises la mÃªme question, câ€™est un signe que lâ€™agent ne fonctionne pas comme prÃ©vu.

**PrÃ©cision :** Ã€ quelle frÃ©quence lâ€™agent produit-il des sorties correctes ou souhaitables ? Les dÃ©finitions de prÃ©cision varient (ex. exactitude de rÃ©solution de problÃ¨mes, prÃ©cision de rÃ©cupÃ©ration dâ€™informations, satisfaction utilisateur). La premiÃ¨re Ã©tape est de dÃ©finir ce que signifie la rÃ©ussite pour votre agent. Vous pouvez suivre la prÃ©cision via des contrÃ´les automatisÃ©s, des scores dâ€™Ã©valuation ou des labels de complÃ©tion de tÃ¢che. Par exemple, marquer les traces comme Â« rÃ©ussi Â» ou Â« Ã©chouÃ© Â».

**MÃ©triques dâ€™Ã‰valuation AutomatisÃ©es :** Vous pouvez aussi configurer des Ã©valuations automatisÃ©es. Par exemple, vous pouvez utiliser un LLM pour Ã©valuer la sortie de lâ€™agent, par ex. si elle est utile, prÃ©cise ou non. Il existe Ã©galement plusieurs bibliothÃ¨ques open source pour Ã©valuer diffÃ©rents aspects de lâ€™agent. Ex. [RAGAS](https://docs.ragas.io/) pour agents RAG ou [LLM Guard](https://llm-guard.com/) pour dÃ©tecter un langage nuisible ou une injection de prompt.

Dans la pratique, une combinaison de ces mÃ©triques offre la meilleure couverture de la santÃ© dâ€™un agent IA. Dans ce [notebook dâ€™exemple du chapitre](./code_samples/10_autogen_evaluation.ipynb), nous vous montrerons Ã  quoi ces mÃ©triques ressemblent dans des exemples rÃ©els, mais dâ€™abord, voyons Ã  quoi ressemble un workflow typique dâ€™Ã©valuation.

## Instrumenter votre Agent

Pour collecter des donnÃ©es de traÃ§age, vous devez instrumenter votre code. Lâ€™objectif est dâ€™instrumenter le code de lâ€™agent afin dâ€™Ã©mettre des traces et mÃ©triques pouvant Ãªtre capturÃ©es, traitÃ©es et visualisÃ©es par une plateforme dâ€™observabilitÃ©.

**OpenTelemetry (OTel) :** [OpenTelemetry](https://opentelemetry.io/) sâ€™est imposÃ© comme un standard industriel pour lâ€™observabilitÃ© LLM. Il fournit un ensemble dâ€™API, SDK et outils pour gÃ©nÃ©rer, collecter et exporter des donnÃ©es tÃ©lÃ©mÃ©triques.

Il existe de nombreuses bibliothÃ¨ques dâ€™instrumentation qui enveloppent les frameworks dâ€™agents existants et facilitent lâ€™export de spans OpenTelemetry vers un outil dâ€™observabilitÃ©. Voici un exemple dâ€™instrumentation dâ€™un agent AutoGen avec la [bibliothÃ¨que dâ€™instrumentation OpenLit](https://github.com/openlit/openlit):

```python
import openlit

openlit.init(tracer = langfuse._otel_tracer, disable_batch = True)
```

Le [notebook dâ€™exemple](./code_samples/10_autogen_evaluation.ipynb) de ce chapitre dÃ©montrera comment instrumenter votre agent AutoGen.

**CrÃ©ation Manuelle de Spans :** Bien que les bibliothÃ¨ques dâ€™instrumentation fournissent une bonne base, il y a parfois des cas oÃ¹ des informations plus dÃ©taillÃ©es ou personnalisÃ©es sont nÃ©cessaires. Vous pouvez crÃ©er des spans manuellement pour ajouter une logique applicative personnalisÃ©e. Plus important encore, ils peuvent enrichir les spans crÃ©Ã©s automatiquement ou manuellement avec des attributs personnalisÃ©s (aussi appelÃ©s tags ou mÃ©tadonnÃ©es). Ces attributs peuvent inclure des donnÃ©es mÃ©tier, des calculs intermÃ©diaires ou tout contexte utile au dÃ©bogage ou Ã  lâ€™analyse, tel que `user_id`, `session_id` ou `model_version`.

Exemple de crÃ©ation manuelle de traces et spans avec le [Langfuse Python SDK](https://langfuse.com/docs/sdk/python/sdk-v3) :

```python
from langfuse import get_client
 
langfuse = get_client()
 
span = langfuse.start_span(name="my-span")
 
span.end()
```

## Ã‰valuation de lâ€™Agent

Lâ€™observabilitÃ© nous fournit des mÃ©triques, mais lâ€™Ã©valuation est le processus dâ€™analyse de ces donnÃ©es (et de rÃ©alisation de tests) pour dÃ©terminer la performance dâ€™un agent IA et les amÃ©liorations possibles. En dâ€™autres termes, une fois que vous avez ces traces et mÃ©triques, comment les utiliser pour juger lâ€™agent et prendre des dÃ©cisions ?

Lâ€™Ã©valuation rÃ©guliÃ¨re est importante car les agents IA sont souvent non dÃ©terministes et peuvent Ã©voluer (via des mises Ã  jour ou un dÃ©rive du comportement du modÃ¨le) â€“ sans Ã©valuation, vous ne sauriez pas si votre â€œagent intelligentâ€ fait rÃ©ellement bien son travail ou sâ€™il a rÃ©gressÃ©.

Il existe deux catÃ©gories dâ€™Ã©valuations pour les agents IA : **Ã©valuation en ligne** et **Ã©valuation hors ligne**. Les deux sont prÃ©cieuses et se complÃ¨tent. Nous commenÃ§ons gÃ©nÃ©ralement par lâ€™Ã©valuation hors ligne, qui est lâ€™Ã©tape minimale nÃ©cessaire avant de dÃ©ployer un agent.

### Ã‰valuation Hors Ligne

![Ã‰lÃ©ments de dataset dans Langfuse](https://langfuse.com/images/cookbook/example-autogen-evaluation/example-dataset.png)

Cela consiste Ã  Ã©valuer lâ€™agent dans un contexte contrÃ´lÃ©, gÃ©nÃ©ralement en utilisant des jeux de donnÃ©es de test, et non des requÃªtes utilisateurs en direct. Vous utilisez des datasets sÃ©lectionnÃ©s oÃ¹ vous connaissez la sortie attendue ou le comportement correct, puis vous exÃ©cutez votre agent dessus.

Par exemple, si vous avez construit un agent de rÃ©solution de problÃ¨mes mathÃ©matiques, vous pourriez disposer dâ€™un [dataset de test](https://huggingface.co/datasets/gsm8k) de 100 problÃ¨mes aux rÃ©ponses connues. Lâ€™Ã©valuation hors ligne se fait souvent lors du dÃ©veloppement (et peut Ãªtre intÃ©grÃ©e dans des pipelines CI/CD) pour vÃ©rifier les amÃ©liorations ou prÃ©venir les rÃ©gressions. Lâ€™avantage est que câ€™est **rÃ©pÃ©table et vous pouvez obtenir des mÃ©triques prÃ©cises de prÃ©cision grÃ¢ce Ã  une vÃ©ritÃ© terrain**. Vous pouvez aussi simuler des requÃªtes utilisateurs et mesurer les rÃ©ponses de lâ€™agent face aux rÃ©ponses idÃ©ales ou utiliser des mÃ©triques automatisÃ©es comme dÃ©crit plus haut.

Le principal dÃ©fi avec lâ€™Ã©valuation hors ligne est de garantir que votre jeu de test est complet et reste pertinent â€“ lâ€™agent pourrait bien performer sur un jeu de test fixe, mais rencontrer des requÃªtes trÃ¨s diffÃ©rentes en production. Par consÃ©quent, vous devez maintenir Ã  jour les jeux de test avec de nouveaux cas limites et exemples reflÃ©tant les scÃ©narios rÃ©els. Un mÃ©lange de petits cas Â« smoke test Â» et de larges jeux dâ€™Ã©valuation est utile : petits ensembles pour des contrÃ´les rapides et plus grands pour des mÃ©triques plus globales.

### Ã‰valuation en Ligne

![Vue dâ€™ensemble des mÃ©triques dâ€™observabilitÃ©](https://langfuse.com/images/cookbook/example-autogen-evaluation/dashboard.png)

Cela dÃ©signe lâ€™Ã©valuation de lâ€™agent dans un environnement rÃ©el, en direct, câ€™est-Ã -dire lors de son usage en production. Lâ€™Ã©valuation en ligne consiste Ã  surveiller la performance de lâ€™agent pendant les interactions utilisateurs rÃ©elles et Ã  analyser continuellement les rÃ©sultats.

Par exemple, vous pouvez suivre les taux de rÃ©ussite, les scores de satisfaction utilisateur ou dâ€™autres mÃ©triques sur le trafic live. Lâ€™avantage de lâ€™Ã©valuation en ligne est quâ€™elle **capte des phÃ©nomÃ¨nes que vous ne pourriez pas anticiper en laboratoire** â€“ vous pouvez observer la dÃ©rive du modÃ¨le au fil du temps (si lâ€™efficacitÃ© de lâ€™agent dÃ©cline avec lâ€™Ã©volution des entrÃ©es) et dÃ©tecter des requÃªtes ou situations inattendues qui nâ€™Ã©taient pas prÃ©sentes dans vos donnÃ©es de test. Elle donne une image rÃ©elle du comportement de lâ€™agent en conditions rÃ©elles.

Lâ€™Ã©valuation en ligne implique souvent la collecte de retours utilisateurs implicites et explicites, comme discutÃ©, et Ã©ventuellement la rÃ©alisation de tests en aveugle ou tests A/B (oÃ¹ une nouvelle version de lâ€™agent tourne en parallÃ¨le pour comparaison avec lâ€™ancienne). Le dÃ©fi est dâ€™obtenir des labels ou scores fiables pour les interactions en direct â€“ vous pouvez vous appuyer sur le feedback utilisateur ou des mÃ©triques en aval (ex. lâ€™utilisateur a-t-il cliquÃ© sur le rÃ©sultat).

### Combinaison des Deux

Les Ã©valuations en ligne et hors ligne ne sont pas exclusives ; elles se complÃ¨tent fortement. Les insights issus de la surveillance en ligne (ex. nouveaux types de requÃªtes oÃ¹ lâ€™agent est faible) peuvent Ãªtre utilisÃ©s pour enrichir et amÃ©liorer les datasets de test hors ligne. Ã€ lâ€™inverse, les agents qui performent bien dans les tests hors ligne peuvent Ãªtre dÃ©ployÃ©s plus sereinement et suivis en ligne.

En fait, de nombreuses Ã©quipes adoptent une boucle :

_Ã©valuer hors ligne -> dÃ©ployer -> monitorer en ligne -> collecter de nouveaux cas dâ€™Ã©chec -> ajouter au dataset hors ligne -> affiner lâ€™agent -> rÃ©pÃ©ter_.

## ProblÃ¨mes Courants

Au fur et Ã  mesure que vous dÃ©ployez des agents IA en production, vous pouvez rencontrer divers dÃ©fis. Voici quelques problÃ¨mes courants et leurs solutions potentielles :

| **ProblÃ¨me**    | **Solution Potentielle**   |
| ------------- | ------------------ |
| Lâ€™agent IA nâ€™exÃ©cute pas les tÃ¢ches de maniÃ¨re cohÃ©rente | - Affiner le prompt donnÃ© Ã  lâ€™agent IA ; Ãªtre clair sur les objectifs.<br>- Identifier si diviser les tÃ¢ches en sous-tÃ¢ches et les gÃ©rer par plusieurs agents aide. |
| Lâ€™agent IA entre dans des boucles continues | - Assurez-vous dâ€™avoir des conditions claires de terminaison pour que lâ€™agent sache quand arrÃªter le processus.<br>- Pour les tÃ¢ches complexes nÃ©cessitant raisonnement et planification, utilisez un modÃ¨le plus grand spÃ©cialisÃ© dans le raisonnement. |
| Les appels aux outils de lâ€™agent IA ne fonctionnent pas bien | - Tester et valider la sortie des outils en dehors du systÃ¨me agent.<br>- Affiner les paramÃ¨tres dÃ©finis, les prompts et le nommage des outils. |
| Le systÃ¨me multi-agent ne fonctionne pas de maniÃ¨re cohÃ©rente | - Affiner les prompts donnÃ©s Ã  chaque agent pour quâ€™ils soient spÃ©cifiques et distincts.<br>- Construire un systÃ¨me hiÃ©rarchique utilisant un agent Â« routeur Â» ou contrÃ´leur pour dÃ©terminer quel agent est le bon. |

Beaucoup de ces problÃ¨mes peuvent Ãªtre identifiÃ©s plus efficacement grÃ¢ce Ã  lâ€™observabilitÃ©. Les traces et mÃ©triques Ã©voquÃ©es prÃ©cÃ©demment aident Ã  localiser prÃ©cisÃ©ment oÃ¹ dans le workflow de lâ€™agent les problÃ¨mes surviennent, rendant le dÃ©bogage et lâ€™optimisation beaucoup plus efficaces.

## Gestion des CoÃ»ts
Voici quelques stratÃ©gies pour gÃ©rer les coÃ»ts de dÃ©ploiement des agents IA en production :

**Utiliser des modÃ¨les plus petits :** Les petits modÃ¨les de langage (SLM) peuvent bien fonctionner pour certains cas d'utilisation agentiques et rÃ©duiront considÃ©rablement les coÃ»ts. Comme mentionnÃ© prÃ©cÃ©demment, construire un systÃ¨me d'Ã©valuation pour dÃ©terminer et comparer les performances par rapport aux modÃ¨les plus grands est la meilleure faÃ§on de comprendre comment un SLM fonctionnera sur votre cas d'utilisation. Envisagez d'utiliser des SLM pour des tÃ¢ches plus simples comme la classification d'intention ou l'extraction de paramÃ¨tres, tout en rÃ©servant les modÃ¨les plus grands pour un raisonnement complexe.

**Utiliser un modÃ¨le de routage :** Une stratÃ©gie similaire consiste Ã  utiliser une diversitÃ© de modÃ¨les et de tailles. Vous pouvez utiliser un LLM/SLM ou une fonction serverless pour router les requÃªtes en fonction de leur complexitÃ© vers les modÃ¨les les mieux adaptÃ©s. Cela aidera Ã©galement Ã  rÃ©duire les coÃ»ts tout en garantissant la performance sur les bonnes tÃ¢ches. Par exemple, router les requÃªtes simples vers des modÃ¨les plus petits et plus rapides, et nâ€™utiliser les modÃ¨les coÃ»teux et volumineux que pour les tÃ¢ches de raisonnement complexes.

**Mise en cache des rÃ©ponses :** Identifier les requÃªtes et tÃ¢ches courantes et fournir les rÃ©ponses avant quâ€™elles ne passent par votre systÃ¨me agentique est un excellent moyen de rÃ©duire le volume de requÃªtes similaires. Vous pouvez mÃªme implÃ©menter un flux pour identifier Ã  quel point une requÃªte est similaire Ã  vos requÃªtes en cache en utilisant des modÃ¨les dâ€™IA plus basiques. Cette stratÃ©gie peut rÃ©duire significativement les coÃ»ts pour les questions frÃ©quemment posÃ©es ou les flux de travail communs.

## Voyons comment cela fonctionne en pratique

Dans le [notebook exemple de cette section](./code_samples/10_autogen_evaluation.ipynb), nous verrons des exemples de lâ€™utilisation des outils dâ€™observabilitÃ© pour surveiller et Ã©valuer notre agent.


### Vous avez dâ€™autres questions sur les agents IA en production ?

Rejoignez le [Discord Microsoft Foundry](https://aka.ms/ai-agents/discord) pour rencontrer dâ€™autres apprenants, assister aux heures de bureau et obtenir des rÃ©ponses Ã  vos questions sur les agents IA.

## LeÃ§on prÃ©cÃ©dente

[Metacognition Design Pattern](../09-metacognition/README.md)

## Prochaine leÃ§on

[Agentic Protocols](../11-agentic-protocols/README.md)

---

<!-- CO-OP TRANSLATOR DISCLAIMER START -->
**Avertissement** :  
Ce document a Ã©tÃ© traduit Ã  lâ€™aide du service de traduction automatique [Co-op Translator](https://github.com/Azure/co-op-translator). Bien que nous nous efforcions dâ€™assurer lâ€™exactitude, veuillez noter que les traductions automatiques peuvent contenir des erreurs ou des inexactitudes. Le document original dans sa langue dâ€™origine doit Ãªtre considÃ©rÃ© comme la source faisant autoritÃ©. Pour les informations critiques, il est recommandÃ© de faire appel Ã  une traduction professionnelle rÃ©alisÃ©e par un humain. Nous dÃ©clinons toute responsabilitÃ© en cas de malentendus ou dâ€™interprÃ©tations erronÃ©es rÃ©sultant de lâ€™utilisation de cette traduction.
<!-- CO-OP TRANSLATOR DISCLAIMER END -->