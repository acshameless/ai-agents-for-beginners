# AI Agents in Production: Observability & Evaluation

[![AI Agents in Production](../../../translated_images/ko/lesson-10-thumbnail.2b79a30773db093e.webp)](https://youtu.be/l4TP6IyJxmQ?si=reGOyeqjxFevyDq9)

AI 에이전트가 실험적 프로토타입에서 실제 응용 프로그램으로 이동함에 따라, 이들의 행동을 이해하고 성능을 모니터링하며 체계적으로 출력을 평가하는 능력이 중요해집니다.

## 학습 목표

이 레슨을 완료한 후, 여러분은 다음을 알게 되거나 이해하게 됩니다:
- 에이전트 관찰성 및 평가의 핵심 개념
- 에이전트의 성능, 비용 및 효율성을 개선하는 기법
- AI 에이전트를 체계적으로 평가하는 방법 및 평가할 항목
- AI 에이전트를 프로덕션에 배포할 때 비용을 제어하는 방법
- AutoGen으로 구축된 에이전트를 계측하는 방법

목표는 여러분의 "블랙 박스" 에이전트를 투명하고 관리 가능하며 신뢰할 수 있는 시스템으로 전환할 수 있는 지식을 제공하는 것입니다.

_**참고:** 안전하고 신뢰할 수 있는 AI 에이전트 배포가 중요합니다. [Building Trustworthy AI Agents](./06-building-trustworthy-agents/README.md) 레슨도 확인해 보세요._

## Traces and Spans

[Langfuse](https://langfuse.com/)나 [Microsoft Foundry](https://learn.microsoft.com/en-us/azure/ai-foundry/what-is-azure-ai-foundry)와 같은 관찰성 도구는 보통 에이전트 실행을 trace와 span으로 표현합니다.

- **Trace**는 시작부터 끝까지의 전체 에이전트 작업을 나타냅니다 (예: 사용자 쿼리 처리).
- **Spans**는 trace 내 개별 단계입니다 (예: 언어 모델 호출 또는 데이터 검색).

![Trace tree in Langfuse](https://langfuse.com/images/cookbook/example-autogen-evaluation/trace-tree.png)

관찰성이 없으면 AI 에이전트는 "블랙 박스"처럼 느껴질 수 있습니다 - 내부 상태와 추론이 불투명하여 문제를 진단하거나 성능을 최적화하기 어렵습니다. 관찰성을 갖추면, 에이전트는 "유리 상자"가 되어 신뢰 구축과 의도한 대로 작동하는지 보장하는 데 중요한 투명성을 제공합니다.

## Why Observability Matters in Production Environments

AI 에이전트를 프로덕션 환경으로 전환하면 새로운 과제와 요구 사항이 나타납니다. 관찰성은 더 이상 "있으면 좋은 것"이 아니라 필수 기능입니다:

*   **디버깅 및 근본 원인 분석**: 에이전트가 실패하거나 예상치 못한 출력을 생성할 때, 관찰성 도구는 오류 원인을 정확히 찾아내는 데 필요한 trace를 제공합니다. 여러 LLM 호출, 도구 상호작용, 조건부 논리가 포함될 수 있는 복잡한 에이전트에서 특히 중요합니다.
*   **지연 시간 및 비용 관리**: AI 에이전트는 토큰 단위 또는 호출 단위로 비용이 부과되는 LLM 및 외부 API에 의존하는 경우가 많습니다. 관찰성은 이러한 호출을 정밀하게 추적하여 지나치게 느리거나 비용이 많이 드는 작업을 식별하는 데 도움을 줍니다. 이를 통해 팀은 프롬프트를 최적화하거나 효율적인 모델을 선택하거나 작업 흐름을 재설계하여 운영 비용을 관리하고 우수한 사용자 경험을 보장할 수 있습니다.
*   **신뢰성, 안전성 및 준수**: 많은 애플리케이션에서 에이전트가 안전하고 윤리적으로 행동하는 것이 중요합니다. 관찰성은 에이전트의 행동과 결정을 기록하는 감사 추적을 제공합니다. 이를 통해 프롬프트 인젝션, 유해한 콘텐츠 생성, 개인 식별 정보(PII)의 잘못된 처리 등의 문제를 감지하고 완화할 수 있습니다. 예를 들어 왜 에이전트가 특정 응답을 했거나 특정 도구를 사용했는지 trace를 검토할 수 있습니다.
*   **지속적인 개선 루프**: 관찰성 데이터는 반복적 개발 프로세스의 기반입니다. 실제 환경에서 에이전트 성능을 모니터링함으로써 개선 포인트를 파악하고, 모델 세부 조정용 데이터를 수집하며, 변경 사항의 영향을 검증할 수 있습니다. 이는 온라인 평가에서 얻은 프로덕션 인사이트가 오프라인 실험과 개선을 이끌어 점진적으로 에이전트 성능을 향상시키는 피드백 루프를 만들어냅니다.

## Key Metrics to Track

에이전트 행동을 모니터링하고 이해하려면 다양한 지표와 신호를 추적해야 합니다. 구체적인 지표는 에이전트의 목적에 따라 다를 수 있지만, 몇몇은 보편적으로 중요합니다.

관찰성 도구가 흔히 모니터하는 주요 지표들은 다음과 같습니다:

**Latency:** 에이전트가 얼마나 빨리 응답하는가? 긴 대기 시간은 사용자 경험에 부정적 영향을 줍니다. 에이전트 실행과 개별 단계에 걸쳐 지연 시간을 측정해야 합니다. 예를 들어, 모든 모델 호출에 20초가 걸리는 에이전트는 더 빠른 모델을 사용하거나 병렬로 모델 호출을 실행해 가속화할 수 있습니다.

**Costs:** 에이전트 실행당 비용은 얼마인가? AI 에이전트는 LLM 호출이나 외부 API 비용에 의존합니다. 도구를 자주 사용하거나 여러 프롬프트를 보내면 비용이 빠르게 증가할 수 있습니다. 예를 들어, 품질 개선이 거의 없는데 LLM을 다섯 번 호출하는 경우, 비용이 정당한지 평가하거나 호출 횟수를 줄이거나 더 저렴한 모델을 사용하는 방안을 고려해야 합니다. 실시간 모니터링은 버그로 인한 과도한 API 반복 등 예상치 못한 비용 급증을 감지하는 데도 도움됩니다.

**Request Errors:** 에이전트가 실패한 요청 수는 얼마인가? 여기엔 API 에러나 도구 호출 실패가 포함됩니다. 프로덕션에서 안정성을 높이려면 폴백(fallback)이나 재시도(retry) 메커니즘을 설정할 수 있습니다. 예를 들어, LLM 제공자 A가 다운되면 백업으로 LLM 제공자 B로 전환하는 식입니다.

**User Feedback:** 직접적인 사용자 평가는 귀중한 통찰을 제공합니다. 명시적 평점(👍좋아요/👎싫어요, ⭐1-5 별점)이나 텍스트 댓글이 이에 해당합니다. 지속적 부정 피드백은 에이전트가 예상대로 작동하지 않는 신호입니다.

**Implicit User Feedback:** 사용자 행동은 명시적 평점 없이도 간접 피드백을 제공합니다. 즉각적인 질문 재작성, 반복 쿼리, 재시도 버튼 클릭 등이 예입니다. 사용자가 반복해서 동일한 질문을 한다면, 이는 에이전트가 기대에 못 미친다는 신호입니다.

**Accuracy:** 에이전트가 올바르거나 바람직한 출력을 얼마나 자주 내는가? 정확도의 정의는 다양합니다(예: 문제 해결 정확도, 정보 검색 정확성, 사용자 만족도). 먼저 여러분 에이전트의 성공 기준을 정의해야 합니다. 자동 검사, 평가 점수, 작업 완료 라벨 등으로 정확도를 추적할 수 있습니다. 예를 들어 trace를 "성공" 또는 "실패"로 표시할 수 있습니다.

**Automated Evaluation Metrics:** 자동 평가를 설정할 수도 있습니다. 예를 들어 LLM을 사용하여 에이전트 출력을 유용성, 정확성 등을 점수화할 수 있습니다. 또한 다양한 에이전트 측면을 점수화하는 여러 오픈 소스 라이브러리가 있습니다. 예: RAG 에이전트를 위한 [RAGAS](https://docs.ragas.io/), 유해 언어 또는 프롬프트 인젝션 탐지를 위한 [LLM Guard](https://llm-guard.com/).

실제로 이러한 지표를 조합하여 AI 에이전트의 상태를 가장 포괄적으로 파악합니다. 이 장의 [예제 노트북](./code_samples/10_autogen_evaluation.ipynb)에서는 이러한 지표가 실제 예제에서 어떻게 나타나는지 보여드리지만, 먼저 전형적인 평가 워크플로우가 어떻게 구성되는지 살펴보겠습니다.

## Instrument your Agent

추적 데이터를 수집하려면 코드를 계측해야 합니다. 목표는 에이전트 코드를 계측하여 trace와 지표를 발생시키고, 이를 관찰성 플랫폼에서 포착, 처리, 시각화할 수 있도록 하는 것입니다.

**OpenTelemetry (OTel):** [OpenTelemetry](https://opentelemetry.io/)는 LLM 관찰성의 산업 표준으로 등장했습니다. 텔레메트리 데이터를 생성, 수집, 내보내는 API, SDK, 도구 모음을 제공합니다.

기존 에이전트 프레임워크를 감싸고 OpenTelemetry span을 관찰성 도구에 쉽게 내보내도록 해주는 계측 라이브러리가 많습니다. 아래는 [OpenLit 계측 라이브러리](https://github.com/openlit/openlit)를 사용해 AutoGen 에이전트를 계측하는 예입니다:

```python
import openlit

openlit.init(tracer = langfuse._otel_tracer, disable_batch = True)
```


이 장의 [예제 노트북](./code_samples/10_autogen_evaluation.ipynb)에서 AutoGen 에이전트를 어떻게 계측하는지 보여줍니다.

**수동 Span 생성:** 계측 라이브러리는 기본적인 기능을 제공하지만, 상세하거나 맞춤 정보가 필요한 경우가 있습니다. 수동으로 span을 생성하여 맞춤 애플리케이션 로직을 추가할 수 있습니다. 더 중요한 것은 자동 또는 수동으로 생성된 span에 비즈니스 특화 데이터, 중간 계산 결과, 디버깅이나 분석에 유용한 문맥(`user_id`, `session_id`, `model_version` 등)과 같은 맞춤 속성(태그 또는 메타데이터라고도 함)을 풍부하게 추가할 수 있다는 점입니다.

[Langfuse Python SDK](https://langfuse.com/docs/sdk/python/sdk-v3)를 사용해 trace와 span을 수동으로 생성하는 예:

```python
from langfuse import get_client
 
langfuse = get_client()
 
span = langfuse.start_span(name="my-span")
 
span.end()
```


## Agent Evaluation

관찰성은 지표를 제공하지만, 평가(evaluation)는 데이터를 분석하고 테스트를 수행하여 AI 에이전트가 얼마나 잘 작동하는지, 어떻게 개선할 수 있는지를 판단하는 과정입니다. 다시 말해 trace와 지표를 얻은 후, 이를 이용해 에이전트를 판단하고 의사결정하는 방법입니다.

정기적인 평가는 중요합니다. AI 에이전트는 종종 비결정적이며 업데이트나 모델 행동 변화에 따라 진화합니다 - 평가 없이는 "스마트 에이전트"가 실제로 잘 작동하는지 아니면 퇴보했는지 알 수 없습니다.

AI 에이전트 평가에는 **온라인 평가**와 **오프라인 평가** 두 가지가 있습니다. 둘 다 중요하며 상호 보완적입니다. 일반적으로 배포 전에 최소한의 단계인 오프라인 평가부터 시작합니다.

### Offline Evaluation

![Dataset items in Langfuse](https://langfuse.com/images/cookbook/example-autogen-evaluation/example-dataset.png)

이는 일반적으로 실시간 사용자 쿼리가 아닌 테스트 데이터셋을 사용하여 통제된 환경에서 에이전트를 평가하는 것을 의미합니다. 예상 출력이나 올바른 동작을 알 수 있는 선별된 데이터셋을 사용하여 에이전트를 실행합니다.

예를 들어, 수학 단어 문제 에이전트를 만들었다면, 알려진 정답이 있는 100개의 문제 데이터셋([test dataset](https://huggingface.co/datasets/gsm8k))을 가지고 있을 수 있습니다. 오프라인 평가는 개발 중에 (CI/CD 파이프라인의 일부일 수 있음) 개선사항을 확인하거나 퇴보를 막기 위해 수행됩니다. 장점은 **반복 가능하며 정답이 있으므로 명확한 정확도 지표를 얻을 수 있다는 점**입니다. 사용자의 쿼리를 시뮬레이션하고, 이상적인 답변과 비교하거나 위에서 설명한 자동 평가 지표를 사용할 수 있습니다.

오프라인 평가의 주요 과제는 테스트 데이터셋이 포괄적이고 최신 상태로 유지되도록 하는 것입니다 – 에이전트가 고정된 테스트 세트에서는 잘 수행되더라도 프로덕션에서 매우 다른 쿼리를 만날 수 있기 때문입니다. 따라서 실세계 시나리오를 반영한 새로운 엣지 케이스와 예제로 테스트 세트를 계속 업데이트해야 합니다. 작은 “스모크 테스트” 세트와 더 큰 평가 세트를 혼합하는 것이 유용합니다: 빠른 점검을 위한 작은 세트와 넓은 범위 성능 측정을 위한 큰 세트입니다.

### Online Evaluation 

![Observability metrics overview](https://langfuse.com/images/cookbook/example-autogen-evaluation/dashboard.png)

이는 실제 사용 중인 라이브 프로덕션 환경에서 에이전트를 평가하는 것을 의미합니다. 온라인 평가는 실제 사용자 상호작용에서 에이전트 성능을 모니터링하고 지속적으로 결과를 분석합니다.

예를 들어 성공률, 사용자 만족도 점수 또는 라이브 트래픽에서 얻은 기타 지표를 추적할 수 있습니다. 온라인 평가의 장점은 **실험실 환경에서는 예상하지 못했던 상황을 포착할 수 있다는 점**입니다 – 입력 패턴 변화로 인해 에이전트 성능 저하(모델 드리프트)를 관찰할 수 있고, 테스트 데이터에 없던 예상치 못한 쿼리나 상황을 발견할 수 있습니다. 이는 야생에서 에이전트가 어떻게 행동하는지 진정한 모습을 제공합니다.

온라인 평가는 앞서 논의한 명시적 및 암묵적 사용자 피드백 수집을 포함하고, 섀도우 테스트나 A/B 테스트(새 버전을 이전 버전과 병렬 실행하여 비교)도 실행할 수 있습니다. 다만 라이브 상호작용에 대해 신뢰할 수 있는 라벨이나 점수를 얻는 것이 까다롭고, 사용자 피드백이나 하위 지표(예: 사용자가 결과를 클릭했는지) 등에 의존할 수 있습니다.

### Combining the two

온라인 및 오프라인 평가는 상호 배타적이지 않으며 매우 상호보완적입니다. 온라인 모니터링에서 얻은 인사이트(예: 에이전트가 부진한 새 유형의 사용자 쿼리)는 오프라인 테스트 데이터셋을 보완하고 개선하는 데 활용할 수 있습니다. 반대로 오프라인 테스트에서 잘 수행된 에이전트는 온라인에 더 자신 있게 배포 및 모니터링할 수 있습니다.

실제로 많은 팀이 다음과 같은 루프를 채택합니다:

_오프라인 평가 -> 배포 -> 온라인 모니터링 -> 새 실패 사례 수집 -> 오프라인 데이터셋에 추가 -> 에이전트 개선 -> 반복_.

## Common Issues

AI 에이전트를 프로덕션에 배포하면서 여러 도전에 직면할 수 있습니다. 다음은 흔한 문제와 그 잠재적 해결책입니다:

| **문제**    | **잠재적 해결책**   |
| ------------- | ------------------ |
| AI 에이전트가 작업을 일관되게 수행하지 않음 | - AI 에이전트에 제공하는 프롬프트를 다듬고 목적을 명확히 함.<br>- 작업을 하위 작업으로 분할하여 여러 에이전트가 처리하도록 하면 도움이 되는지 파악. |
| AI 에이전트가 계속 루프에 빠짐  | - 종료 조건을 명확히 하여 에이전트가 언제 프로세스를 종료할지 알도록 함.<br>- 추론 및 계획이 필요한 복잡한 작업은 추론 작업에 특화된 더 큰 모델 사용. |
| AI 에이전트 도구 호출 성능 저하   | - 에이전트 시스템 외부에서 도구 출력 테스트 및 검증.<br>- 정의된 매개변수, 프롬프트, 도구 명칭을 다듬음.  |
| 다중 에이전트 시스템이 일관성 없음 | - 각 에이전트에 주는 프롬프트를 구체적이고 서로 구분되게 다듬음.<br>- 어떤 에이전트가 적절한지 결정하는 “라우팅” 또는 제어 에이전트를 사용해 계층적 시스템 구축. |

많은 문제는 관찰성이 갖추어져 있을 때 더 효과적으로 식별할 수 있습니다. 앞서 논의한 trace와 지표는 에이전트 작업 흐름의 어느 부분에서 문제가 발생하는지 정확히 파악하여 디버깅과 최적화를 훨씬 효율적으로 만듭니다.

## Managing Costs
생산 환경에 AI 에이전트를 배포하는 비용을 관리하는 몇 가지 전략은 다음과 같습니다:

**작은 모델 사용:** 작은 언어 모델(SLM)은 특정 에이전트 활용 사례에서 좋은 성능을 발휘하며 비용을 크게 줄일 수 있습니다. 앞서 언급한 바와 같이, 평가 시스템을 구축하여 성능을 확인하고 대형 모델과 비교하는 것이 SLM이 사용 사례에서 얼마나 잘 작동할지 이해하는 가장 좋은 방법입니다. 간단한 작업(예: 의도 분류나 파라미터 추출)에는 SLM을 사용하고, 복잡한 추론에는 대형 모델을 사용하는 것을 고려하세요.

**라우터 모델 사용:** 유사한 전략으로 다양한 크기와 모델을 사용하는 방법이 있습니다. 요청의 복잡도에 따라 LLM/SLM 또는 서버리스 함수를 사용해 적합한 모델로 요청을 라우팅할 수 있습니다. 이를 통해 비용을 줄이는 동시에 적절한 작업에서 성능을 보장할 수 있습니다. 예를 들어 간단한 쿼리는 더 작고 빠른 모델에 라우팅하고, 복잡한 추론 작업에는 비용이 큰 대형 모델만 사용합니다.

**응답 캐싱:** 일반적인 요청과 작업을 식별하여 에이전트 시스템을 거치기 전에 응답을 제공하는 것은 유사한 요청의 양을 줄이는 좋은 방법입니다. 더 기본적인 AI 모델을 사용해 요청이 캐시된 요청과 얼마나 유사한지 식별하는 흐름도 구현할 수 있습니다. 이 전략은 자주 묻는 질문이나 일반적인 워크플로우에 대해 비용을 크게 줄일 수 있습니다.

## 실제로 어떻게 작동하는지 살펴보기

[이 섹션의 예제 노트북](./code_samples/10_autogen_evaluation.ipynb)에서 관찰 도구를 사용해 에이전트를 모니터링하고 평가하는 방법의 예를 살펴보겠습니다.


### 생산 환경 AI 에이전트에 대해 더 궁금한 점이 있나요?

[Microsoft Foundry Discord](https://aka.ms/ai-agents/discord)에 가입하여 다른 학습자들과 만나고, 오피스 아워에 참석하며 AI 에이전트 관련 질문에 대한 답변을 받아보세요.

## 이전 강의

[메타인지 디자인 패턴](../09-metacognition/README.md)

## 다음 강의

[에이전틱 프로토콜](../11-agentic-protocols/README.md)

---

<!-- CO-OP TRANSLATOR DISCLAIMER START -->
**면책 조항**:  
이 문서는 AI 번역 서비스 [Co-op Translator](https://github.com/Azure/co-op-translator)를 사용하여 번역되었습니다. 정확성을 위해 최선을 다하고 있으나, 자동 번역은 오류나 부정확한 내용이 포함될 수 있음을 유의하시기 바랍니다. 원본 문서의 원어 버전을 권위 있는 자료로 간주해야 합니다. 중요한 정보에 대해서는 전문 인력의 번역을 권장합니다. 본 번역 사용으로 인해 발생하는 오해나 잘못된 해석에 대해서는 당사가 책임지지 않습니다.
<!-- CO-OP TRANSLATOR DISCLAIMER END -->