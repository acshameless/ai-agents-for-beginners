# AI Agents in Production：可观测性与评估

[![生产环境中的 AI 代理](../../../translated_images/zh-CN/lesson-10-thumbnail.2b79a30773db093e.webp)](https://youtu.be/l4TP6IyJxmQ?si=reGOyeqjxFevyDq9)

随着 AI 代理从实验性原型走向真实世界的应用，理解其行为、监控其性能并系统性地评估其输出变得尤为重要。

## 学习目标

完成本课后，你将知道/理解：
- 代理可观测性与评估的核心概念
- 提高代理性能、成本和有效性的技术
- 系统性地评估你的 AI 代理的内容与方法
- 在将 AI 代理部署到生产环境时如何控制成本
- 如何对使用 AutoGen 构建的代理进行埋点/监控

目标是使你具备将“黑箱”代理转变为透明、可管理且可靠系统的知识。

_**注意：** 部署安全且值得信赖的 AI 代理非常重要。也请查看 [构建值得信赖的 AI 代理](./06-building-trustworthy-agents/README.md) 课程。_

## 跟踪（Trace）与跨度（Span）

可观测性工具（例如 [Langfuse](https://langfuse.com/) 或 [Microsoft Foundry](https://learn.microsoft.com/en-us/azure/ai-foundry/what-is-azure-ai-foundry)）通常将代理运行表示为跟踪和跨度。

- **Trace** 表示从开始到结束的完整代理任务（例如处理一个用户查询）。
- **Spans** 是跟踪中的单个步骤（例如调用语言模型或检索数据）。

![Trace tree in Langfuse](https://langfuse.com/images/cookbook/example-autogen-evaluation/trace-tree.png)

没有可观测性时，AI 代理会让人感觉像一个“黑箱”——其内部状态和推理过程是不透明的，难以诊断问题或优化性能。有了可观测性，代理变成“玻璃箱”，提供的透明性对于建立信任并确保其按预期运行至关重要。

## 为什么在生产环境中可观测性很重要

将 AI 代理迁移到生产环境会引入一系列新的挑战和要求。可观测性不再是“可有可无”的，而是一项关键能力：

*   **调试与根因分析**：当代理失败或产生意外输出时，可观测性工具提供的跟踪可帮助定位错误根源。这在可能涉及多个 LLM 调用、工具交互和条件逻辑的复杂代理中尤其重要。
*   **延迟与成本管理**：AI 代理通常依赖按 token 或按调用计费的 LLM 和其他外部 API。可观测性可以精确跟踪这些调用，帮助识别过慢或过于昂贵的操作。这使团队能够优化提示、选择更高效的模型或重新设计工作流，以管理运营成本并保证良好用户体验。
*   **信任、安全与合规**：在许多应用中，确保代理的行为安全且符合伦理非常重要。可观测性提供了代理行为和决策的审计轨迹。这可用于检测和缓解提示注入、有害内容生成或个人身份信息（PII）处理不当等问题。例如，你可以审查跟踪以了解代理为何给出某个响应或使用特定工具。
*   **持续改进循环**：可观测性数据是迭代开发流程的基础。通过监控代理在真实世界的表现，团队可以识别改进点、收集用于微调模型的数据，并验证变更的影响。这创建了一个反馈循环：来自在线评估的生产洞察用于指导离线实验和改进，从而逐步提升代理性能。

## 需要跟踪的关键指标

为了监控和理解代理行为，应跟踪一系列指标和信号。具体指标可能会根据代理的用途而有所不同，但有些是普遍重要的。

下面是可观测性工具通常监控的一些常见指标：

**延迟：** 代理响应的速度有多快？长时间等待会对用户体验产生负面影响。你应通过跟踪代理运行来衡量任务和各个步骤的延迟。例如，一个所有模型调用总共需要 20 秒的代理，可以通过使用更快的模型或并行运行模型调用来加速。

**成本：** 每次代理运行的费用是多少？AI 代理依赖按 token 或外部 API 调用计费的 LLM。频繁使用工具或多次提示会迅速增加成本。例如，如果代理调用 LLM 五次只换来边际质量提升，那么你必须评估成本是否合理，或是否可以减少调用次数或使用更便宜的模型。实时监控还可以帮助识别意外激增（例如导致过度 API 循环的 bug）。

**请求错误：** 代理失败的请求有多少？这可以包括 API 错误或工具调用失败。为了让代理在生产环境中更健壮，你可以设置回退或重试。例如，如果 LLM 提供商 A 出现故障，你可以切换到 LLM 提供商 B 作为备用。

**用户反馈：** 实施直接的用户评估能提供有价值的洞察。这可以包括明确的评分（👍/👎、⭐1-5 星）或文本评论。持续的负面反馈应引起你的注意，因为这表明代理没有按预期工作。

**隐式用户反馈：** 即使没有明确评分，用户行为也能提供间接反馈。这可以包括立即重新措辞问题、重复查询或点击重试按钮。例如，如果你发现用户反复提出相同的问题，这表明代理没有按预期工作。

**准确性：** 代理生成正确或期望输出的频率如何？准确性的定义各不相同（例如问题求解正确性、信息检索准确性、用户满意度）。第一步是定义你的代理的成功标准。你可以通过自动化检查、评估分数或任务完成标签来跟踪准确性。例如，将跟踪标记为“成功”或“失败”。

**自动化评估指标：** 你也可以设置自动评估。例如，你可以使用 LLM 为代理的输出打分，如判断其是否有帮助、是否准确等。也有若干开源库可以帮助评估代理的不同方面，例如用于 RAG 代理的 [RAGAS](https://docs.ragas.io/) 或用于检测有害语言或提示注入的 [LLM Guard](https://llm-guard.com/)。

在实践中，结合这些指标能最好地覆盖 AI 代理的健康状况。在本章的 [示例笔记本](./code_samples/10_autogen_evaluation.ipynb) 中，我们将展示这些指标在真实示例中的表现，但首先，我们将了解典型的评估工作流。

## 为代理添加监控

要收集跟踪数据，你需要对代码进行埋点/监控。目标是为代理代码添加埋点，使其发出可被可观测性平台捕获、处理和可视化的跟踪和指标。

**OpenTelemetry (OTel)：** [OpenTelemetry](https://opentelemetry.io/) 已成为 LLM 可观测性的行业标准。它提供了一套用于生成、收集和导出遥测数据的 API、SDK 和工具。

有许多封装现有代理框架并使将 OpenTelemetry spans 导出到可观测性工具变得简单的埋点库。下面是使用 [OpenLit instrumentation library](https://github.com/openlit/openlit) 对 AutoGen 代理进行埋点的示例：

```python
import openlit

openlit.init(tracer = langfuse._otel_tracer, disable_batch = True)
```

本章的 [示例笔记本](./code_samples/10_autogen_evaluation.ipynb) 将演示如何对你的 AutoGen 代理进行埋点。

**手动创建 Span：** 虽然埋点库提供了良好的基线，但常常会有需要更详细或自定义信息的情况。你可以手动创建 spans 以添加自定义应用逻辑。更重要的是，你可以为自动或手动创建的 spans 增加自定义属性（也称为标签或元数据）。这些属性可以包括业务特定的数据、中间计算结果或任何对调试或分析有用的上下文，例如 `user_id`、`session_id` 或 `model_version`。

使用 [Langfuse Python SDK](https://langfuse.com/docs/sdk/python/sdk-v3) 手动创建跟踪和跨度的示例：

```python
from langfuse import get_client
 
langfuse = get_client()
 
span = langfuse.start_span(name="my-span")
 
span.end()
```

## 代理评估

可观测性为我们提供了指标，但评估是分析这些数据（并执行测试）以判断 AI 代理表现如何及如何改进的过程。换句话说，一旦你有了这些跟踪和指标，如何利用它们来评判代理并做出决策？

定期评估很重要，因为 AI 代理通常是非确定性的并且会随时间演变（通过更新或模型行为漂移）——如果没有评估，你无法知道你的“智能代理”是否真正完成了它的工作，或是否出现了回归。

AI 代理的评估可分为两类：**在线评估**和**离线评估**。两者都很有价值，且互为补充。我们通常先从离线评估开始，因为这是部署任何代理前的最低必要步骤。

### 离线评估

![Dataset items in Langfuse](https://langfuse.com/images/cookbook/example-autogen-evaluation/example-dataset.png)

这涉及在受控环境中评估代理，通常使用测试数据集，而不是实时用户查询。你使用策划的数据集，其中你知道预期输出或正确行为，然后在这些数据上运行代理。

例如，如果你构建了一个数学应用题代理，你可能有一个包含 100 道已知答案问题的[测试数据集](https://huggingface.co/datasets/gsm8k)。离线评估通常在开发过程中进行（并可以作为 CI/CD 管道的一部分）以检查改进或防止回归。其好处是 **可重复，并且由于有真实标签，你可以获得明确的准确性指标**。你也可以模拟用户查询，并将代理的响应与理想答案进行比较，或使用前文所述的自动化指标。

离线评估的关键挑战是确保你的测试数据集具有全面性并保持相关性——代理可能在固定测试集上表现良好，但在生产环境中遇到非常不同的查询。因此，你应保持测试集的更新，加入反映真实场景的新边缘案例和示例。混合使用小型“冒烟测试”用例和较大的评估集是有用的：小集合用于快速检查，较大集合用于更广泛的性能度量。

### 在线评估

![Observability metrics overview](https://langfuse.com/images/cookbook/example-autogen-evaluation/dashboard.png)

这指的是在实时、真实环境中评估代理，即在生产中实际使用期间进行评估。在线评估涉及监控代理在真实用户交互中的表现并持续分析结果。

例如，你可能会跟踪实时流量中的成功率、用户满意度评分或其他指标。在线评估的优势在于它 **能捕捉到你在实验室环境中可能无法预见的情况**——你可以观察到模型随时间的漂移（如果输入模式变化导致代理效果下降），并捕获测试数据中没有的意外查询或情况。它提供了代理在真实环境中行为的真实图景。

在线评估通常涉及收集隐式和显式用户反馈（如前所述），也可能运行影子测试或 A/B 测试（在并行运行新版本代理以与旧版本比较）。挑战在于对实时交互获取可靠标签或评分可能很困难——你可能需要依赖用户反馈或下游指标（例如用户是否点击了结果）。

### 两者结合

在线与离线评估并非互斥；它们高度互补。来自在线监控的洞察（例如代理在某类新型用户查询上表现不佳）可用于扩展和改进离线测试数据集。反之，在离线测试中表现良好的代理可以更有信心地部署并在线监控。

事实上，许多团队采用下面的循环：

_先在离线评估 -> 部署 -> 在线监控 -> 收集新的失败案例 -> 将其加入离线数据集 -> 优化代理 -> 重复_。

## 常见问题

在将 AI 代理部署到生产环境时，你可能会遇到各种挑战。以下是一些常见问题及其可能的解决方案：

| **问题**    | **可能的解决方案**   |
| ------------- | ------------------ |
| AI Agent not performing tasks consistently | - 优化提供给 AI 代理的提示；明确目标。<br>- 识别是否可以将任务划分为子任务并由多个代理分别处理。 |
| AI Agent running into continuous loops  | - 确保你有明确的终止条件，使代理知道何时停止该流程。<br>- 对于需要推理和规划的复杂任务，使用擅长推理任务的更大模型。 |
| AI Agent tool calls are not performing well   | - 在代理系统之外测试并验证工具的输出。<br>- 优化工具的参数定义、提示和命名。  |
| Multi-Agent system not performing consistently | - 优化分配给每个代理的提示，确保它们具体且彼此区分。<br>- 构建一个分层系统，使用“路由”或控制器代理来决定哪个代理是正确的。 |

许多这些问题在启用可观测性后可以更有效地识别。我们前面讨论的跟踪和指标有助于准确定位代理工作流中问题发生的位置，从而使调试和优化更加高效。

## 成本管理
以下是一些管理将 AI 代理部署到生产环境成本的策略：

**Using Smaller Models:** 小型语言模型 (SLMs) 在某些代理化用例中可以表现良好，并能显著降低成本。如前所述，构建一个评估系统以确定并比较与更大型模型的性能，是了解 SLM 在特定用例中表现的最佳方式。考虑在更简单的任务（如意图分类或参数提取）中使用 SLMs，同时将更大型模型保留用于复杂推理。

**Using a Router Model:** 类似的策略是使用不同种类和规模的模型。您可以使用 LLM/SLM 或无服务器函数，根据请求的复杂性将请求路由到最合适的模型。这同样有助于降低成本，同时确保在适当的任务上获得良好性能。例如，将简单查询路由到更小、更快速的模型，只有在复杂推理任务时才使用昂贵的大模型。

**Caching Responses:** 识别常见请求和任务，并在它们通过您的代理化系统之前提供响应，是减少类似请求数量的好方法。您甚至可以实现一个流程，使用更基础的 AI 模型来识别一个请求与缓存请求的相似程度。对于常见问题或常见工作流，这一策略可以显著降低成本。

## 让我们看看这在实践中如何运作

在本节的[示例笔记本](./code_samples/10_autogen_evaluation.ipynb)中，我们将看到如何使用可观测性工具来监控和评估我们的代理的示例。

### 关于生产环境中的 AI 代理还有更多问题吗？

加入 [Microsoft Foundry Discord](https://aka.ms/ai-agents/discord) 与其他学习者交流，参加答疑时间，并获得关于 AI 代理的问题解答。

## 上一课

[元认知设计模式](../09-metacognition/README.md)

## 下一课

[代理协议](../11-agentic-protocols/README.md)

---

<!-- CO-OP TRANSLATOR DISCLAIMER START -->
免责声明：
本文件已使用 AI 翻译服务 [Co-op Translator](https://github.com/Azure/co-op-translator) 进行翻译。尽管我们力求准确，但请注意，自动翻译可能包含错误或不准确之处。原始语言的文档应被视为权威来源。对于关键信息，建议采用专业人工翻译。对于因使用本翻译而产生的任何误解或曲解，我们不承担任何责任。
<!-- CO-OP TRANSLATOR DISCLAIMER END -->