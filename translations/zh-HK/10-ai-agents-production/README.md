# 產品化中的 AI 代理人：可觀察性與評估

[![產品化中的 AI 代理人](../../../translated_images/zh-HK/lesson-10-thumbnail.2b79a30773db093e.webp)](https://youtu.be/l4TP6IyJxmQ?si=reGOyeqjxFevyDq9)

隨著 AI 代理人從實驗性原型轉向真實世界應用，理解其行為、監控其效能以及系統性地評估其輸出能力變得非常重要。

## 學習目標

完成本課程後，您將能夠/了解：
- 代理人的可觀察性與評估核心概念
- 提升代理人性能、成本及效果的技術
- 什麼是以及如何系統性評估您的 AI 代理人
- 如何在部署 AI 代理人到生產環境時控制成本
- 如何為使用 AutoGen 建立的代理人執行監測

目標是讓您掌握將「黑盒子」代理人轉化為透明、可管理且可靠系統的知識。

_**注意：** 部署安全且值得信賴的 AI 代理人非常重要。請參考[建立可信賴 AI 代理人](./06-building-trustworthy-agents/README.md)課程。_

## 跟蹤與跨度 (Traces and Spans)

像 [Langfuse](https://langfuse.com/) 或 [Microsoft Foundry](https://learn.microsoft.com/en-us/azure/ai-foundry/what-is-azure-ai-foundry) 這樣的可觀察性工具通常將代理人運行表示為跟蹤（trace）和跨度（span）。

- **Trace** 代表一個完整的代理人任務從開始到結束（例如處理一個用戶查詢）。
- **Spans** 是該跟蹤中的個別步驟（例如呼叫語言模型或檢索資料）。

![Langfuse 中的跟蹤樹](https://langfuse.com/images/cookbook/example-autogen-evaluation/trace-tree.png)

若無可觀察性，AI 代理人往往如「黑盒子」般 — 其內部狀態及推理過程不透明，難以診斷問題或優化效能。有了可觀察性，代理人變成「玻璃盒子」，提供透明度，對構建信任並確保代理人按預期運行至關重要。

## 為何可觀察性在生產環境中重要

將 AI 代理人轉入生產環境會帶來一系列新的挑戰和需求。可觀察性不再是「可有可無」，而是關鍵能力：

*   **除錯與根因分析**：當代理人失敗或產生意外輸出時，可觀察性工具提供所需的跟蹤以鎖定錯誤來源。這在涉及多次 LLM 呼叫、工具互動及條件邏輯的複雜代理中尤其重要。
*   **延遲與成本管理**：AI 代理人常依賴按令牌數或呼叫次數計費的 LLM 和其他外部 API。可觀察性允許精確追蹤這些呼叫，有助識別過慢或過貴的操作。這讓團隊能優化提示詞、選擇更高效模型或重新設計工作流程，以管理運營成本並確保良好用戶體驗。
*   **信任、安全與合規**：在多數應用中，確保代理人行為安全且合乎道德非常重要。可觀察性提供代理人行為及決策的審計軌跡，用於偵測與減輕例如提示注入、產生有害內容或錯誤處理個人識別資訊 (PII) 等問題。例如，您可以查看跟蹤了解代理人為何給出某個回應或使用特定工具。
*   **持續改進循環**：可觀察性數據是迭代開發流程的基礎。藉由監測代理人在真實世界中的表現，團隊可找出改進點、收集調優模型的數據並驗證變更效果。這形成反饋循環，從線上評估的生產洞察反哺離線實驗與優化，進而持續提升代理人性能。

## 需追蹤的關鍵指標

要監控並了解代理人的行為，應追蹤一系列指標與信號。具體指標會依代理人目標不同而異，但以下一些是通用且重要的。

下列為可觀察性工具常監控的指標：

**延遲時間 (Latency)：** 代理人響應速度有多快？等待時間過長會影響用戶體驗。您應透過追蹤代理人運行來測量任務及個別步驟的延遲時間。例如，一個代理人所有模型呼叫總共花 20 秒，可透過改用更快模型或平行執行模型呼叫來加速。

**成本：** 每次代理運行費用多少？AI 代理人依賴按令牌計費的 LLM 呼叫或外部 API。頻繁使用工具或多次提示會迅速增加成本。例如，如果代理人呼叫 LLM 五次只為了微幅提升質量，就須評估是否值得，或應減少呼叫次數或換更便宜模型。即時監控也能發現異常尖峰（如 BUG 導致大量 API 循環呼叫）。

**請求錯誤數：** 代理人失敗多少請求？可包含 API 錯誤或工具呼叫失敗。為提升代理人產線的魯棒度，您可設定備援或重試策略。例如，若 LLM 供應商 A 離線，則切換為 LLM 供應商 B 作備用。

**用戶反饋：** 實施直接用戶評價提供寶貴見解。包含明確評分（👍讚/👎踩，⭐1-5 星）或文字評論。持續負面反饋應引起警覺，代表代理人表現未達預期。

**隱性用戶反饋：** 用戶行為提供間接回饋，即使無明確評分也可觀察。包括即時重述問題、重複查詢或點擊重試按鈕。例如用戶反覆問相同問題是代理人未達標的訊號。

**準確率 (Accuracy)：** 代理人產出正確或理想結果的頻率有多高？準確率定義會不同（例如問題解決正確率、資訊檢索精確度、用戶滿意度）。第一步是定義代理人成功樣貌。您可以通過自動檢查、評分或任務完成標籤追蹤準確度。例如將跟蹤標記為「成功」或「失敗」。

**自動評估指標：** 您亦可建立自動評估機制。例如使用 LLM 對代理人輸出進行打分，如是否有幫助、準確等。還有多個開源庫輔助您評分代理人不同面向。例如針對 RAG 代理的 [RAGAS](https://docs.ragas.io/) 或用於偵測有害語言及提示注入的 [LLM Guard](https://llm-guard.com/)。

實務上，這些指標結合使用能最佳監控 AI 代理人狀況。本章[示範筆記本](./code_samples/10_autogen_evaluation.ipynb)將演示這些指標於實例中之呈現方式，但首先我會介紹典型評估工作流程。

## 代理人監測執行

為收集追蹤數據，您需要對程式碼進行監測執行。目標是令代理人程式碼輸出可由可觀察平台捕獲、處理和視覺化的跟蹤與指標。

**OpenTelemetry (OTel)：** [OpenTelemetry](https://opentelemetry.io/) 已成為 LLM 可觀察性的業界標準。其提供產生、收集和匯出遙測數據的一套 API、SDK 與工具。

現有許多監測執行庫能包裹代理人框架，簡化將 OpenTelemetry 跨度匯出到可觀察工具的流程。以下為使用 [OpenLit 監測庫](https://github.com/openlit/openlit) 為 AutoGen 代理人執行監測的範例：

```python
import openlit

openlit.init(tracer = langfuse._otel_tracer, disable_batch = True)
```


本章[示範筆記本](./code_samples/10_autogen_evaluation.ipynb)將演示如何監測您的 AutoGen 代理人。

**手動創建跨度：** 雖然監測執行庫提供良好基線，但有時需要更詳細或自訂資訊。您可以手動創建跨度以加入自定義應用邏輯。更重要的是，可以為自動或手動創建的跨度附加自定義屬性（又稱標籤或元數據）。這些屬性可包含業務特定資料、中間計算結果，或任何有助於調試或分析的上下文，例如 `user_id`、`session_id` 或 `model_version`。

以下為使用 [Langfuse Python SDK](https://langfuse.com/docs/sdk/python/sdk-v3) 手動創建跟蹤與跨度範例：

```python
from langfuse import get_client
 
langfuse = get_client()
 
span = langfuse.start_span(name="my-span")
 
span.end()
```


## 代理人評估

可觀察性提供指標，而評估則為分析這些數據（與執行測試）以判斷 AI 代理人表現及改進空間的過程。換言之，獲得跟蹤與指標後，您如何利用它們來評價代理人並做決策？

定期評估很重要，因為 AI 代理人通常非確定性且會演進（透過更新或模型行為漂移）—無評估將無法判斷「聰明代理人」是否真的妥善工作或是否退步。

AI 代理人的評估分兩類：**離線評估**與**線上評估**。兩者皆有價值且相輔相成。通常先從離線評估開始，因為這是部署代理人前的最低必要步驟。

### 離線評估

![Langfuse 中的資料集項目](https://langfuse.com/images/cookbook/example-autogen-evaluation/example-dataset.png)

這是指在受控環境下評估代理人，通常使用測試資料集，而非現場用戶查詢。使用您已知預期輸出或正確行為的精選資料集，再將代理人運行於此。

例如，您建立數學文字題代理人，可能有一個含100道題且帶有標準答案的[測試資料集](https://huggingface.co/datasets/gsm8k)。離線評估多在開發階段執行（也可納入 CI/CD 流程）以檢查改進或防止退化。好處是**可重複，且因有地面真相而獲得明確準確率指標**。您或許還會模擬用戶查詢，並衡量代理回應與理想答案的差異，或如前述使用自動化指標。

離線評估大挑戰在於確保測試資料集完整且持續貼近實際 — 代理可能在固定測試集上表現良好，但在生產中面對截然不同查詢。因此須持續更新測試集，引入反映真實場景的邊緣案例與範例。組合小型「煙霧測試」與較大型評估集具實用性：小型快速檢查，大型涵蓋廣泛指標。

### 線上評估

![可觀察性指標總覽](https://langfuse.com/images/cookbook/example-autogen-evaluation/dashboard.png)

指在現場實際使用、真實環境中評估代理人。線上評估涉及持續監控代理人在真實用戶互動中的表現並分析結果。

例如，您可能追蹤成功率、用戶滿意度分數等實時指標。線上評估優點在於**捕捉實驗室環境中預期不到的狀況**—可觀察模型隨時間漂移（代理效能因輸入模式改變而退化），並發現測試資料中未包含的意外查詢或情況。它反映代理人在真實世界的真實表現。

線上評估往往結合隱性與顯性用戶反饋，如前所述，可能還會進行陰影測試或 A/B 測試（新版本代理與舊版本並行運行做比較）。挑戰是取得可靠標籤或評分相當困難 — 您或許得依賴用戶反饋或後續指標（例如用戶是否點擊結果）。

### 結合兩者

線上與離線評估非彼此排斥，反而密切互補。線上監控洞察（如代理人效能不佳的新型用戶查詢）可用於擴充及改善離線測試資料集。反之，在離線測試良好的代理，部署後可更有信心在生產持續監控。

事實上，許多團隊採用評估循環：

_離線評估 -> 部署 -> 線上監控 -> 蒐集失敗案例 -> 加入離線資料集 -> 精煉代理 -> 重複。_

## 常見問題

部署 AI 代理人至生產環境時，可能會遇到各種挑戰。以下列出常見問題及潛在解決方案：

| **問題**    | **潛在解決方案**   |
| ------------- | ------------------ |
| AI 代理人任務執行不一致 | - 精煉給代理人的提示詞，明確目標。<br>- 找出可將任務拆分成子任務，由多個代理人分別處理的地方。 |
| AI 代理人陷入持續循環  | - 設定清晰終止條件，讓代理人知道何時停止流程。<br>- 針對需推理與規劃的複雜任務，使用擅長推理的大型模型。 |
| AI 代理人工具呼叫效能不佳   | - 在代理系統外測試及驗證工具輸出。<br>- 精煉工具參數、提示詞及命名設定。 |
| 多代理系統表現不穩定 | - 精煉給每個代理的提示詞，確保彼此具體且相異。<br>- 建立分層系統，使用「路由」或控制代理決定正確代理。 |

裝置可觀察性後，能更有效識別許多問題。前述的跟蹤與指標有助精確定位代理工作流程中的問題，讓除錯與優化更高效。

## 成本管理
以下是部署 AI 代理到生產環境時管理成本的一些策略：

**使用較小的模型：**小型語言模型（SLMs）在某些具代理性的用例中表現良好，並能大幅降低成本。如前所述，建立評估系統以確定並比較其與較大模型的性能，是了解 SLM 在您的用例中表現如何的最佳方法。考慮將 SLM 用於較簡單的任務，例如意圖分類或參數萃取，同時將較大模型保留給複雜推理。

**使用路由模型：**類似策略是使用多樣化的模型和大小。您可以使用 LLM/SLM 或無服務函數根據複雜度將請求路由至最合適的模型。這同時有助於降低成本並確保在適當任務上的性能。例如，將簡單查詢路由至較小且速度較快的模型，僅對複雜推理任務使用昂貴的大模型。

**快取回應：**識別常見請求和任務，並在它們通過您的代理系統之前提供回應，是減少相似請求數量的好方法。您甚至可以實作流程，利用較基礎的 AI 模型來判斷請求與快取請求的相似程度。此策略能大幅減少常見問題或通用工作流程的成本。

## 讓我們看看這在實務中的應用

在本節的[範例筆記本](./code_samples/10_autogen_evaluation.ipynb)中，我們將看到如何使用可觀察性工具來監控和評估我們的代理。

### 關於生產環境中的 AI 代理有更多問題？

加入 [Microsoft Foundry Discord](https://aka.ms/ai-agents/discord)，與其他學習者交流，參加開放時間並獲得您的 AI 代理問題解答。

## 前一課

[後設認知設計模式](../09-metacognition/README.md)

## 下一課

[具代理性協議](../11-agentic-protocols/README.md)

---

<!-- CO-OP TRANSLATOR DISCLAIMER START -->
**免責聲明**：  
本文件由 AI 翻譯服務 [Co-op Translator](https://github.com/Azure/co-op-translator) 翻譯而成。雖然我們致力於確保準確性，但請注意自動翻譯可能包含錯誤或不準確之處。原始文件的母語版本應被視為權威來源。對於重要資訊，建議採用專業人類翻譯。我們不對因使用本翻譯而引起的任何誤解或誤譯承擔責任。
<!-- CO-OP TRANSLATOR DISCLAIMER END -->