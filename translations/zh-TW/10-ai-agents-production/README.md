# 在生產環境中的 AI 代理：可觀察性與評估

[![在生產環境中的 AI 代理](../../../translated_images/zh-TW/lesson-10-thumbnail.2b79a30773db093e.webp)](https://youtu.be/l4TP6IyJxmQ?si=reGOyeqjxFevyDq9)

隨著 AI 代理從實驗原型轉向真實世界應用，理解它們的行為、監控其效能，以及系統性地評估其輸出變得非常重要。

## 學習目標

完成本課程後，你將知道/了解：
- 代理可觀察性與評估的核心概念
- 改善代理效能、成本與有效性的技術
- 系統性評估 AI 代理的項目與方法
- 在將 AI 代理部署到生產環境時如何控制成本
- 如何為使用 AutoGen 建立的代理加入監控

目標是讓你具備將「黑盒」代理轉變為透明、可管理且可靠系統的知識。

_**注意：** 部署安全且值得信賴的 AI 代理非常重要。也請參考 [建立可信賴的 AI 代理](./06-building-trustworthy-agents/README.md) 的課程。_

## 追蹤（Trace）與區段（Span）

可觀察性工具（例如 [Langfuse](https://langfuse.com/) 或 [Microsoft Foundry](https://learn.microsoft.com/en-us/azure/ai-foundry/what-is-azure-ai-foundry)）通常將代理執行表示為追蹤與區段。

- **追蹤（Trace）** 表示從開始到結束的一整個代理任務（例如處理使用者查詢）。
- **區段（Spans）** 是追蹤中的個別步驟（例如呼叫語言模型或檢索資料）。

![Langfuse 的追蹤樹](https://langfuse.com/images/cookbook/example-autogen-evaluation/trace-tree.png)

沒有可觀察性時，AI 代理可能感覺像一個「黑盒」— 它的內部狀態與推理過程不透明，使得診斷問題或優化效能變得困難。有了可觀察性，代理便成為「玻璃盒」，提供透明度，這對建立信任並確保其按預期運作至關重要。

## 為何在生產環境中可觀察性很重要

將 AI 代理遷移到生產環境會引入一系列新的挑戰與需求。可觀察性不再是「可有可無」，而是一項關鍵能力：

*   **除錯與根因分析**：當代理失敗或產生意外輸出時，可觀察性工具提供追蹤資料來精準定位錯誤來源。這在可能涉及多次 LLM 呼叫、工具互動與條件邏輯的複雜代理中尤其重要。
*   **延遲與成本管理**：AI 代理常依賴按 token 或按次收費的 LLM 與其他外部 API。可觀察性允許精確追蹤這些呼叫，以識別過慢或過於昂貴的操作。這讓團隊能優化提示、選擇更有效率的模型，或重新設計工作流程，以管理營運成本並確保良好的使用者體驗。
*   **信任、安全與合規**：在許多應用中，確保代理行為安全且合乎道德非常重要。可觀察性提供代理行為與決策的審計軌跡，可用於偵測與緩解如提示注入、產生有害內容或錯誤處理包含個人識別資訊（PII）的問題。例如，你可以檢視追蹤紀錄以了解代理為何給出特定回應或使用特定工具。
*   **持續改進迴圈**：可觀察性資料是迭代開發流程的基礎。透過監控代理在真實世界的表現，團隊可以識別改進空間、收集用於微調模型的資料，並驗證變更的影響。這會建立一個回饋迴圈，讓來自線上評估的生產洞見回饋到離線實驗與優化，逐步提升代理表現。

## 要追蹤的關鍵指標

為了監控與理解代理行為，應追蹤一系列指標與信號。具體指標會依代理目的不同而不同，但有些指標是普遍重要的。

以下是可觀察性工具常監控的幾個常見指標：

**Latency:** 代理回應有多快？長時間等待會負面影響使用者體驗。你應透過追蹤代理執行來衡量任務與各個步驟的延遲。例如，如果代理在所有模型呼叫上總共花了 20 秒，可以考慮使用更快的模型或並行執行模型呼叫來加速。

**Costs:** 每次代理執行的花費是多少？AI 代理依賴按 token 或按次計費的 LLM 呼叫或外部 API。頻繁的工具使用或多次提示會迅速增加成本。例如，如果代理呼叫 LLM 五次只是為了邊際品質提升，你就必須評估這樣的成本是否合理，或是否可以減少呼叫次數或使用較便宜的模型。即時監控也能幫助識別意外的暴增（例如錯誤導致過多的 API 迴圈）。

**Request Errors:** 代理失敗的請求有多少？這可以包括 API 錯誤或工具呼叫失敗。為了在生產中使代理更具韌性，你可以設置回退或重試機制。例如，如果 LLM 供應商 A 宕機，你可以切換到 LLM 供應商 B 作為備援。

**User Feedback:** 實作直接的使用者評價能提供寶貴洞見。這可以包括明確評分（👍thumbs-up/👎down、⭐1-5 星）或文字回饋。一致性的負面回饋應該警示你，這表示代理未如預期運作。

**Implicit User Feedback:** 即使沒有明確評分，使用者行為也會提供間接回饋。這可以包括立刻重新表述問題、重複查詢或點擊重試按鈕。例如，如果你看到使用者反覆問同樣的問題，這就是代理未如預期工作的跡象。

**Accuracy:** 代理產生正確或期望輸出的頻率如何？準確性的定義會因情境而異（例如問題解決正確性、資訊檢索準確度、使用者滿意度）。第一步是定義對你的代理而言何謂成功。你可以透過自動檢查、評分或任務完成標籤來追蹤準確性。例如，將追蹤標記為 `succeeded` 或 `failed`。

**Automated Evaluation Metrics:** 你也可以設置自動評估。例如，可以使用 LLM 為代理的輸出打分（例如是否有幫助、是否準確）。也有幾個開源函式庫可以幫助你評分代理的不同面向，例如 [RAGAS](https://docs.ragas.io/) 用於 RAG 代理，或 [LLM Guard](https://llm-guard.com/) 用於偵測有害語言或提示注入。

在實務上，這些指標的組合能最好地涵蓋 AI 代理的健康狀況。在本章的 [範例筆記本](./code_samples/10_autogen_evaluation.ipynb) 中，我們會展示這些指標在真實範例中的樣貌，但首先，我們會了解典型的評估工作流程是如何運作的。

## 為你的代理加入監控

要收集追蹤資料，你需要為程式加入監控（instrumentation）。目標是為代理程式加入可發出追蹤與指標的邏輯，讓可觀察性平台能擷取、處理與視覺化這些資料。

**OpenTelemetry (OTel):** [OpenTelemetry](https://opentelemetry.io/) 已成為 LLM 可觀察性的業界標準。它提供一組 API、SDK 與工具，用於產生、收集與匯出遙測資料。

有許多封裝現有代理框架並便於將 OpenTelemetry spans 匯出到可觀察性工具的監控函式庫。下面是一個使用 [OpenLit instrumentation library](https://github.com/openlit/openlit) 為 AutoGen 代理加入監控的範例：

```python
import openlit

openlit.init(tracer = langfuse._otel_tracer, disable_batch = True)
```

本章的 [範例筆記本](./code_samples/10_autogen_evaluation.ipynb) 將示範如何為你的 AutoGen 代理加入監控。

**Manual Span Creation:** 雖然監控函式庫提供了不錯的基礎，但在很多情況下你可能需要更詳細或自訂的資訊。你可以手動建立 spans 以加入自訂的應用程式邏輯。更重要的是，你可以為自動或手動建立的 spans 增添自訂屬性（也稱為標籤或 Metadata）。這些屬性可以包含特定業務的資料、中間計算結果，或任何有助於除錯或分析的上下文，例如 `user_id`、`session_id` 或 `model_version`。

以下是使用 [Langfuse Python SDK](https://langfuse.com/docs/sdk/python/sdk-v3) 手動建立追蹤與區段的範例：

```python
from langfuse import get_client
 
langfuse = get_client()
 
span = langfuse.start_span(name="my-span")
 
span.end()
```

## 代理評估

可觀察性提供我們指標，但評估是分析那些資料（並執行測試）以判定 AI 代理表現與改進方式的過程。換句話說，一旦你有了追蹤與指標，你該如何利用它們來評斷代理並做出決策？

定期評估很重要，因為 AI 代理常是非決定性的且可能會演變（透過更新或模型行為漂移）— 若沒有評估，你無法判定你的「智慧代理」是否真正在做好工作，或是否有退化。

AI 代理的評估可分為兩類：**線上評估（online evaluation）** 與 **離線評估（offline evaluation）**。兩者都很有價值，且互為補充。我們通常從離線評估開始，因為在部署任何代理之前，這是必要的最低步驟。

### 離線評估

![Langfuse 的資料集項目](https://langfuse.com/images/cookbook/example-autogen-evaluation/example-dataset.png)

這指的是在受控環境中評估代理，通常使用測試資料集，而非即時的使用者查詢。你會使用已策畫的資料集，在那裡你知道期望輸出或正確行為，然後在這些資料上執行代理。

例如，如果你建立了一個數學文字題代理，你可能會有一個包含 100 個已知答案問題的 [測試資料集](https://huggingface.co/datasets/gsm8k)。離線評估通常在開發階段進行（也可以成為 CI/CD 管線的一部分），用來檢查改進或防止回歸。其好處是「可重複，且由於你有正確答案，因此可以取得明確的準確性指標」。你也可以模擬使用者查詢並將代理回應與理想答案比較，或使用前述的自動化指標。

離線評估的主要挑戰在於確保你的測試資料集是全面且持續相關的—代理可能在固定測試集上表現良好，但在生產中遇到非常不同的查詢。因此，你應該持續更新測試集，加入反映真實世界情境的新邊界案例與範例。混合使用小型「煙霧測試」與較大的評估集很有用：小型集用於快速檢查，較大集則用於更廣泛的效能評估。

### 線上評估

![可觀察性指標總覽](https://langfuse.com/images/cookbook/example-autogen-evaluation/dashboard.png)

這是指在真實、即時環境中評估代理，也就是在生產中實際使用時。線上評估涉及監控代理在真實使用者互動中的表現並持續分析結果。

例如，你可能會追蹤成功率、使用者滿意度分數或其他在真實流量上的指標。線上評估的優勢在於「捕捉實驗室環境中可能未預見的情況」—你可以觀察隨時間的模型漂移（如果輸入模式改變導致代理效能下降）並發現測試資料中沒有出現的意外查詢或情境。它提供了代理在實際運作中的真實表現圖景。

線上評估通常會收集隱含與明確的使用者回饋（如先前所述），也可能進行 shadow 測試或 A/B 測試（讓新版本代理與舊版並行運作以進行比較）。挑戰在於為即時互動取得可靠的標籤或分數可能較困難—你可能需要依賴使用者回饋或下游指標（例如使用者是否點擊結果）。

### 結合兩者

線上與離線評估並非互斥；它們高度互補。來自線上監控的洞見（例如代理在某些新型使用者查詢上表現不佳）可用於擴充與改進離線測試資料集。相反地，在離線測試上表現良好的代理，可以更有信心地部署並在線上監控。

事實上，許多團隊採用一個迴圈：

_離線評估 -> 部署 -> 線上監控 -> 收集新的失敗案例 -> 加入離線資料集 -> 精煉代理 -> 重複_。

## 常見問題

當你將 AI 代理部署到生產中，可能會遇到各種挑戰。以下是一些常見問題及其潛在解決方案：

| **問題**    | **潛在解決方案**   |
| ------------- | ------------------ |
| AI 代理無法一致執行任務 | - 精煉給 AI 代理的提示；明確定義目標。<br>- 找出可將任務拆分為子任務並由多個代理處理的情況。 |
| AI 代理陷入持續迴圈  | - 確保有明確的終止條件與判斷，讓代理知道何時結束流程。<br>- 對於需要推理與規劃的複雜任務，使用更大且專精於推理任務的模型。 |
| AI 代理的工具呼叫效能不佳   | - 在代理系統之外測試並驗證工具的輸出。<br>- 精煉工具的參數、提示與命名。  |
| 多代理系統執行不穩定 | - 精煉給每個代理的提示，確保它們的任務具體且彼此區隔。<br>- 建立分層系統，使用「路由」或控制代理來決定哪個代理是正確的。 |

許多這些問題在有可觀察性機制時能更有效被識別。我們先前討論的追蹤與指標有助於精確指出代理工作流程中哪個環節出現問題，讓除錯與優化更有效率。

## 管理成本
以下是一些管理將 AI 代理部署到生產環境成本的策略：

**Using Smaller Models:** 小型語言模型（SLMs）在某些具代理性的使用案例中表現良好，並能顯著降低成本。如前所述，建立一個評估系統來判斷並比較與較大型模型的表現，是了解 SLM 在你的使用情境中表現如何的最佳方式。考慮將 SLM 用於像意圖分類或參數抽取等較簡單的任務，同時將較大型模型保留給需要複雜推理的情境。

**Using a Router Model:** 類似的策略是使用多種模型與不同規模。你可以使用 LLM/SLM 或無伺服器函式來根據請求的複雜度將其導向最適合的模型。這樣不僅有助於降低成本，還能確保在合適的任務上達到良好效能。例如，將簡單的查詢導向較小且較快速的模型，僅在需複雜推理的任務才使用昂貴的大型模型。

**Caching Responses:** 識別常見的請求和任務，並在它們進入你的代理系統之前先提供回應，是減少類似請求數量的好方法。你甚至可以使用較基礎的 AI 模型實作一個流程，來判斷某個請求與快取請求的相似度。這項策略可以顯著降低常見問題或常見工作流程的成本。

## 讓我們看看這在實務上如何運作

在 [本節的範例 notebook](./code_samples/10_autogen_evaluation.ipynb) 中，我們將看到如何使用可觀測性工具來監控並評估我們的代理的範例。


### 對於在生產環境部署 AI 代理有更多問題嗎？

加入 [Microsoft Foundry 的 Discord 伺服器](https://aka.ms/ai-agents/discord) 與其他學習者交流，參加辦公時間並取得你關於 AI 代理的問題解答。

## 上一課

[元認知設計模式](../09-metacognition/README.md)

## 下一課

[代理協定](../11-agentic-protocols/README.md)

---

<!-- CO-OP TRANSLATOR DISCLAIMER START -->
免責聲明：
本文件係透過 AI 翻譯服務 [Co-op Translator](https://github.com/Azure/co-op-translator) 進行翻譯。雖然我們力求準確，但請注意，自動翻譯可能包含錯誤或不精確之處。原始語言版本應被視為具權威性的來源。對於關鍵資訊，建議尋求專業人工翻譯。我們對因使用本翻譯而產生的任何誤解或誤譯不負任何責任。
<!-- CO-OP TRANSLATOR DISCLAIMER END -->